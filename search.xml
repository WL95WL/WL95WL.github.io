<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[hadoop之hdfs]]></title>
    <url>%2F2020%2F01%2F16%2Fhadoop%E4%B9%8Bhdfs%2F</url>
    <content type="text"><![CDATA[介绍Hadoop分布式文件系统（HDFS）是一种旨在在商品硬件上运行的分布式文件系统。它与现有的分布式文件系统有许多相似之处。但是，与其他分布式文件系统的区别很明显。HDFS具有高度的容错能力，旨在部署在低成本硬件上。HDFS提供对应用程序数据的高吞吐量访问，并且适用于具有大数据集的应用程序。HDFS放宽了一些POSIX要求，以实现对文件系统数据的流式访问。HDFS最初是作为Apache Nutch Web搜索引擎项目的基础结构而构建的。HDFS是Apache Hadoop Core项目的一部分。项目URL是http://hadoop.apache.org/。 HDFS Architecture 源自:http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html HDFS has a master/slave architecture. An HDFS cluster consists of a single NameNode, ==a master server that manages the file system namespace and regulates access to files by clients==. In addition, there are a number of DataNodes, usually one per node in the cluster, which manage storage attached to the nodes that they run on. HDFS exposes a file system namespace and allows user data to be stored in files. Internally, a file is split into one or more blocks and these blocks are stored in a set of DataNodes. The NameNode executes file system namespace operations like opening, closing, and renaming files and directories. ==It also determines the mapping of blocks to DataNodes.== The DataNodes are responsible for serving read and write requests from the file system’s clients. The DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode. HDFS具有主/从体系结构。HDFS群集由单个NameNode和管理文件系统名称空间并控制客户端对文件的访问的主服务器组成。此外，还有许多数据节点，通常是集群中每个节点一个，用于管理与它们所运行的节点相连的存储。HDFS公开了文件系统名称空间，并允许用户数据存储在文件中。在内部，文件被分成一个或多个块，这些块存储在一组DataNode中。NameNode执行文件系统名称空间操作，例如打开，关闭和重命名文件和目录。它还确定块到DataNode的映射。DataNode负责处理来自文件系统客户端的读写请求。DataNode还会执行块创建，删除 namenode:存储系统的元数据(用于描述数据的数据,==内存==),例如 文件命名空间/block到datanode的映射.负责管理datanode datanode:用于存储数据块的节点.负责响应客户端对块的读写请求,向namenode汇报自己块信息. block:数据块,是对文件拆分的最小单位,表示一个切分尺度默认值128MB,每个数据块的默认副本因子是3通过dfs.replication进行配置,用户可以通过dfs.blocksize设置块大小 rack机架,使用机架对存储节点做物理编排,用于优化存储和计算 12345&gt; [root@CentOS ~]# hdfs dfsadmin -printTopology 查看机架&gt; Rack: /default-rack&gt; 192.168.169.139:50010 (CentOS)&gt; &gt; 为什么说HDFS不擅长存储小文件? 文件 namenode占用(内存) datanode占用磁盘 128MB 单个文件 1个block元数据信息 128MB * 副本因子 128MB 10000个文件 10000个block元数据信息 128MB * 副本因子 因为Namenode是使用单机的内存存储元数据,因此导致namenode内存紧张. NameNode和Secondary Namenode的关系? fsimage 文件镜像 edits 日志文件 辅助NameNode整理Edits和Fsimage文件,加速NameNode启动过程. https://blog.csdn.net/WYpersist/article/details/79840776 安全模式启动时，NameNode进入一个特殊的状态，称为安全模式。当NameNode处于安全模式状态时，不会发生数据块的复制。NameNode从数据节点接收心跳和Blockreport消息。Blockreport包含DataNode托管的数据块列表。每个块都有指定的最小副本数。当已使用NameNode检入该数据块的最小副本数时，该块被视为已安全复制。在使用可配置百分比的安全复制数据块通过NameNode签入（再加上30秒）后，NameNode退出安全模式状态。然后，它确定仍少于指定数量的副本的数据块列表（如果有）。然后，NameNode将这些块复制到其他DataNode。 文件系统元数据的持久性HDFS命名空间由NameNode存储。NameNode使用一个称为EditLog的事务日志来永久记录文件系统元数据发生的每个更改。例如，在HDFS中创建一个新文件将导致NameNode将一条记录插入到EditLog中，以表明这一点。同样，更改文件的复制因子会导致将新记录插入到EditLog中。NameNode使用其本地主机OS文件系统中的文件来存储EditLog。整个文件系统名称空间（包括块到文件的映射和文件系统属性）存储在名为FsImage的文件中。FsImage也作为文件存储在NameNode的本地文件系统中。 NameNode在内存中保留整个文件系统名称空间和文件Blockmap的图像。当NameNode启动或由可配置的阈值触发检查点时，它会从磁盘读取FsImage和EditLog，将EditLog中的所有事务应用于FsImage的内存中表示形式，并将此新版本刷新为磁盘上的新FsImage。然后，它可以截断旧的EditLog，因为其事务已应用于持久性FsImage。此过程称为检查点。检查点的目的是通过获取文件系统元数据的快照并将其保存到FsImage来确保HDFS具有文件系统元数据的一致视图。即使读取FsImage效率很高，但直接对FsImage进行增量编辑效率也不高。我们无需为每个编辑修改FsImage，而是将编辑保留在Editlog中。在检查点期间，来自Editlog的更改将应用于FsImage。可以在给定的时间间隔触发检查点（``以秒表示的dfs.namenode.checkpoint.period，或者在累积一定数量的文件系统事务之后（dfs.namenode.checkpoint.txns）。如果同时设置了这两个属性，则要达到的第一个阈值将触发检查点。 DataNode将HDFS数据存储在其本地文件系统中的文件中。DataNode不了解HDFS文件。它将每个HDFS数据块存储在其本地文件系统中的单独文件中。DataNode不会在同一目录中创建所有文件。而是使用启发式方法确定每个目录的最佳文件数，并适当创建子目录。在同一目录中创建所有本地文件不是最佳选择，因为本地文件系统可能无法有效地支持单个目录中的大量文件。当DataNode启动时，它将扫描其本地文件系统，生成与每个本地文件相对应的所有HDFS数据块的列表，并将此报告发送到NameNode。该报告称为Blockreport。 通讯协议所有HDFS通信协议都位于TCP / IP协议之上。客户端建立与NameNode计算机上可配置TCP端口的连接。它将ClientProtocol与NameNode进行通信。DataNode使用DataNode协议与NameNode对话。远程过程调用（RPC）抽象包装了客户端协议和DataNode协议。按照设计，NameNode永远不会启动任何RPC。相反，它仅响应由DataNode或客户端发出的RPC请求。 坚固性HDFS的主要目标是即使出现故障也能可靠地存储数据。三种常见的故障类型是NameNode故障，DataNode故障和网络分区。 数据磁盘故障，心跳和复制每个DataNode定期向NameNode发送心跳消息。网络分区可能导致一部分DataNode失去与NameNode的连接。NameNode通过缺少心跳消息来检测到这种情况。NameNode将没有最近心跳的DataNode标记为已死，并且不会将任何新的IO请求转发给它们。已注册到失效DataNode的任何数据不再可用于HDFS。DataNode死亡可能导致某些块的复制因子降至其指定值以下。NameNode不断跟踪需要复制的块，并在必要时启动复制。由于许多原因，可能需要进行重新复制：DataNode可能不可用，副本可能损坏，DataNode上的硬盘可能发生故障， 标记DataNode失效的超时时间保守地长（默认情况下超过10分钟），以避免由DataNode的状态震荡引起的复制风暴。用户可以设置较短的时间间隔以将DataNode标记为过时，并通过配置来避免对性能敏感的工作负载进行读和/或写时出现过时的节点。 集群再平衡HDFS体系结构与数据重新平衡方案兼容。如果DataNode的可用空间低于某个阈值，则方案可能会自动将数据从一个DataNode移至另一个DataNode。如果对特定文件的需求突然增加，则方案可能会动态创建其他副本并重新平衡群集中的其他数据。这些类型的数据重新平衡方案尚未实现。 数据的完整性从DataNode提取的数据块可能会损坏。由于存储设备故障，网络故障或软件故障，可能会导致这种损坏。HDFS客户端软件对HDFS文件的内容执行校验和检查。客户端创建HDFS文件时，它将计算文件每个块的校验和，并将这些校验和存储在同一HDFS命名空间中的单独的隐藏文件中。客户端检索文件内容时，它将验证从每个DataNode接收的数据是否与存储在关联的校验和文件中的校验和匹配。如果不是，则客户端可以选择从另一个具有该块副本的DataNode中检索该块。 元数据磁盘故障FsImage和EditLog是HDFS的中央数据结构。这些文件损坏可能导致HDFS实例无法正常运行。因此，可以将NameNode配置为支持维护FsImage和EditLog的多个副本。FsImage或EditLog的任何更新都会导致FsImages和EditLogs中的每个同步更新。FsImage和EditLog的多个副本的这种同步更新可能会降低NameNode可以支持的每秒名称空间事务处理的速度。但是，这种降级是可以接受的，因为即使HDFS应用程序本质上是数据密集型的，但它们也不是元数据密集型的。当NameNode重新启动时，它将选择要使用的最新一致的FsImage和EditLog。 增强抗故障能力的另一种方法是使用多个NameNode来启用高可用性，这些NameNode可以在NFS上使用共享存储，也可以使用分布式编辑日志（称为Journal）。推荐使用后者。 快照快照支持在特定时间存储数据副本。快照功能的一种用法可能是将损坏的HDFS实例回滚到以前已知的良好时间点。 资料组织数据块HDFS旨在支持非常大的文件。与HDFS兼容的应用程序是处理大型数据集的应用程序。这些应用程序仅写入一次数据，但读取一次或多次，并要求以流速度满足这些读取要求。HDFS支持文件上一次写入多次读取的语义。HDFS使用的典型块大小为128 MB。因此，HDFS文件被切成128 MB的块，并且如果可能的话，每个块将驻留在不同的DataNode上。 复制管道当客户端将数据写入复制因子为3的HDFS文件时，NameNode使用复制目标选择算法检索DataNode列表。该列表包含将托管该块副本的DataNode。然后，客户端写入第一个DataNode。第一个DataNode开始分批接收数据，将每个部分写入其本地存储库，然后将该部分传输到列表中的第二个DataNode。第二个DataNode依次开始接收数据块的每个部分，将该部分写入其存储库，然后将该部分刷新到第三个DataNode。最后，第三个DataNode将数据写入其本地存储库。因此，DataNode可以从流水线中的前一个接收数据，同时将数据转发到流水线中的下一个。从而， 辅助功能可以通过许多不同的方式从应用程序访问HDFS。HDFS本身就为应用程序提供了FileSystem Java API。一本Java API的C语言包装和REST API也是可用的。此外，HTTP浏览器还可以用于浏览HDFS实例的文件。通过使用NFS网关，HDFS可以作为客户端本地文件系统的一部分安装。 FS外壳HDFS允许以文件和目录的形式组织用户数据。它提供了一个称为FS shell的命令行界面，该界面可让用户与HDFS中的数据进行交互。该命令集的语法类似于用户已经熟悉的其他shell（例如bash，csh）。以下是一些示例操作/命令对： 行动 命令 创建一个名为/ foodir的目录`` bin / hadoop dfs -mkdir / foodir 删除名为/ foodir的目录`` bin / hadoop fs -rm -R / foodir 查看名为/foodir/myfile.txt的文件的内容`` bin / hadoop dfs -cat /foodir/myfile.txt FS Shell适用于需要脚本语言与存储的数据进行交互的应用程序。 DFS管理员DFSAdmin命令集用于管理HDFS群集。这些是仅由HDFS管理员使用的命令。以下是一些示例操作/命令对： 行动 命令 将群集置于安全模式 bin / hdfs dfsadmin -safemode输入 生成数据节点列表 bin / hdfs dfsadmin -report 重新启用或停用DataNode bin / hdfs dfsadmin -refreshNodes 浏览器界面典型的HDFS安装会将Web服务器配置为通过可配置的TCP端口公开HDFS命名空间。这允许用户使用Web浏览器浏览HDFS命名空间并查看其文件的内容。 填海工程文件删除和取消删除如果启用垃圾箱配置，则不会立即从HDFS中删除由FS Shell删除的文件。而是，HDFS将其移动到回收站目录（每个用户在/ user / &lt;用户名&gt; /。Trash下都有自己的回收站目录）。只要文件保留在垃圾桶中，就可以快速恢复。 最新删除的文件被移动到当前的回收站目录（/ user / &lt;用户名&gt; ``/。Trash / Current），并在可配置的间隔内，HDFS创建检查点（在/ user / &lt;用户名&gt; ``/。Trash ``/ &lt;date&gt;下）用于当前回收站目录中的文件，并在过期时删除旧的检查点。有关垃圾的检查点，请参见FS shell的expunge命令。 在垃圾桶中到期后，NameNode将从HDFS命名空间中删除该文件。文件的删除导致与文件关联的块被释放。请注意，在用户删除文件的时间与HDFS中相应的可用空间增加的时间之间可能会有明显的时间延迟。 下面是一个示例，它将显示FS Shell如何从HDFS删除文件。我们在目录delete下创建了2个文件（test1和test2） 1234567&gt; $ hadoop fs -mkdir -p删除/测试1&gt; $ hadoop fs -mkdir -p删除/测试2&gt; $ hadoop fs -ls删除/&gt; 找到2项&gt; drwxr-xr-x-hadoop hadoop 0 2015-05-08 12:39删除/测试1&gt; drwxr-xr-x-hadoop hadoop 0 2015-05-08 12:40删除/ test2&gt; 我们将删除文件test1。下面的注释显示文件已移至“废纸directory”目录。 123&gt; $ hadoop fs -rm -r删除/测试1&gt; 移动到：hdfs：// localhost：8020 / user / hadoop / .hdfs：// localhost：8020 / user / hadoop / delete / test1移至垃圾桶。垃圾桶/当前&gt; 现在我们要使用skipTrash选项删除文件，该选项不会将文件发送到Trash。它将从HDFS中完全删除。 123&gt; $ hadoop fs -rm -r -skip垃圾删除/测试2&gt; 删除删除/测试2&gt; 现在我们可以看到“废纸contains”目录仅包含文件test1。 1234&gt; $ hadoop fs -ls。垃圾桶/当前/用户/ hadoop /删除/&gt; 找到1项\&gt; drwxr-xr-x-hadoop hadoop 0 2015-05-08 12:39 .Trash / Current / user / hadoop / delete / test1&gt; 因此，文件test1进入垃圾箱，文件test2被永久删除。 减少复制因子当减少文件的复制因子时，NameNode选择可以删除的多余副本。下一个心跳将此信息传输到DataNode。然后，DataNode删除相应的块，并且相应的可用空间出现在群集中。同样，在setReplication API调用完成与群集中的可用空间出现之间可能会有时间延迟。 HDFS Shell 12345678910111213141516171819202122232425&gt; [root@CentOS ~]# hdfs dfs -help&gt; Usage: hadoop fs [generic options]&gt; [-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]&gt; [-cat [-ignoreCrc] &lt;src&gt; ...]&gt; [-checksum &lt;src&gt; ...]&gt; [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]&gt; [-copyFromLocal [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]&gt; [-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]&gt; [-cp [-f] [-p | -p[topax]] &lt;src&gt; ... &lt;dst&gt;]&gt; [-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]&gt; [-help [cmd ...]]&gt; [-ls [-d] [-h] [-R] [&lt;path&gt; ...]]&gt; [-mkdir [-p] &lt;path&gt; ...]&gt; [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]&gt; [-moveToLocal &lt;src&gt; &lt;localdst&gt;]&gt; [-mv &lt;src&gt; ... &lt;dst&gt;]&gt; [-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]&gt; [-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]&gt; [-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]&gt; [-tail [-f] &lt;file&gt;]&gt; [-text [-ignoreCrc] &lt;src&gt; ...]&gt; [-touchz &lt;path&gt; ...]&gt; [-usage [cmd ...]]&gt; &gt; appendToFile 追加文件 12&gt; [root@CentOS ~]# hdfs dfs -appendToFile /root/install.log /aa.log&gt; cat 查看 hdfs dfs 或Hadoop fs 12&gt; [root@CentOS ~]# hadoop fs -cat /aa.log&gt; checksum 查看文件签名 类似于liunx的MD5sum 123&gt; [root@CentOS ~]# hdfs dfs -checksum /aa.log&gt; /aa.log MD5-of-0MD5-of-512CRC32C 000002000000000000000000fa622ce196be3efd11475d6b55af76d2&gt; chmod 修改权限 12345&gt; [root@CentOS ~]# hadoop fs -chmod -R u+x /&gt; [root@CentOS ~]# hadoop fs -ls -R /&gt; -rwxr--r-- 1 root supergroup 17630 2019-01-03 09:05 /aa.log&gt; -rwxr--r-- 1 root supergroup 175262413 2019-01-02 20:29 /jdk-8u171-linux-x64.rpm&gt; copyFromLocal 从本地copy文件到hdfs/ 从hdfs copy文件到本地 copyToLocal 123&gt; [root@CentOS ~]# hdfs dfs -copyFromLocal|-put install.log / # 上传&gt; [root@CentOS ~]# hdfs dfs -copyToLocal|-get /install.log ~/ # 下载&gt; cp 123&gt; [root@CentOS ~]# hdfs dfs -mkdir -p /demo/dir&gt; [root@CentOS ~]# hdfs dfs -cp /install.log /demo/dir&gt; moveFromLocal|moveToLocal 剪贴 1234567&gt; [root@CentOS ~]# hdfs dfs -moveFromLocal ~/install.log /&gt; [root@CentOS ~]# ls&gt; anaconda-ks.cfg hadoop-2.6.0_x64.tar.gz install.log.syslog&gt; [root@CentOS ~]# hdfs dfs -moveToLocal /install.log ~/ # 目前还没有实现&gt; moveToLocal: Option '-moveToLocal' is not implemented yet.&gt; &gt; rm 123&gt; [root@CentOS ~]# hdfs dfs -rm -r -f /install.log&gt; &gt; mv 123&gt; [root@CentOS ~]# hdfs dfs -mv /install.log /bb.log&gt; &gt; cat|text|tail tail -f 监视文件输出 123&gt; [root@CentOS ~]# hdfs dfs -text /cc.log&gt; &gt; touchz 创建文件 123&gt; [root@CentOS ~]# hdfs dfs -touchz /cc.log&gt; &gt; distcp hdfs系统之间的copy 123&gt; [root@CentOS ~]# hadoop distcp hdfs://CentOS:9000/aa.log hdfs://CentOS:9000/demo/dir^C&gt; &gt; 更多参考:http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html#appendToFile 开启 HDFS 的回收站etc/hadoop/core-site.xml 123456&gt; &lt;property&gt;&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt;&gt; &lt;value&gt;1&lt;/value&gt;&gt; &lt;/property&gt;&gt; &gt; 设置1分钟延迟,1分钟以后被删除文件会被系统彻底删除.防止用户误操作 12345678&gt; [root@CentOS ~]# hdfs dfs -rm -r -f /bb.log&gt; 19/01/03 12:27:08 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 1 minutes, Emptier interval = 0 minutes.&gt; Moved: 'hdfs://CentOS:9000/bb.log' to trash at: hdfs://CentOS:9000/user/root/.Trash/Current&gt; &gt; [root@CentOS ~]# hdfs dfs -rm -r -f -skipTrash /aa.log&gt; Deleted /aa.log&gt; &gt; JAVA API 操作 HDFSMaven 12345678910111213141516171819&gt; &lt;dependency&gt;&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;&gt; &lt;version&gt;2.6.0&lt;/version&gt;&gt; &lt;/dependency&gt;&gt; &gt; &lt;dependency&gt;&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;&gt; &lt;version&gt;2.6.0&lt;/version&gt;&gt; &lt;/dependency&gt;&gt; &gt; &lt;dependency&gt;&gt; &lt;groupId&gt;junit&lt;/groupId&gt;&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt;&gt; &lt;version&gt;4.12&lt;/version&gt;&gt; &lt;/dependency&gt;&gt; &gt; Windows开发Hadoop应用环境配置 解压hadoop安装包到C:/ 将winutils.exe和hadoop.dll拷贝到hadoop的bin目录下 在windows配置HADOOP_HOME环境变量 重启开发工具idea,否则开发工具无法识别HADOOP_HOME 在Windows主机配置CentOS的主机名和IP的映射关系 C:\Windows\System32\drivers\etc\hosts 123&gt; 192.168.169.139 CentOS&gt; &gt; HDFS权限不足导致写失败? 123456&gt; org.apache.hadoop.security.AccessControlException: Permission denied: user=HIAPAD, access=WRITE, inode="/":root:supergroup:drwxr-xr-x&gt; at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkFsPermission(FSPermissionChecker.java:271)&gt; at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:257)&gt; ...&gt; &gt; 解决方案 方案1 etc/hadoop/hdfs-site.xml 123456&gt; &lt;property&gt;&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt;&gt; &lt;value&gt;false&lt;/value&gt;&gt; &lt;/property&gt;&gt; &gt; 关闭HDFS文件权限检查,修改完成后,重启HDFS服务 方案2 1234&gt; -DHADOOP_USER_NAME=root&gt; &gt; &gt; 设置JAVA虚拟机启动参数java XXX -Dxx=xxx 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112&gt; import org.apache.hadoop.conf.Configuration;&gt; import org.apache.hadoop.fs.*;&gt; import org.apache.hadoop.io.IOUtils;&gt; import org.apache.hadoop.util.Progressable;&gt; import org.junit.After;&gt; import org.junit.Before;&gt; import org.junit.Test;&gt; import static org.junit.Assert.*;&gt; import java.io.*;&gt; &gt; public class TestHDFSDemo &#123;&gt; private FileSystem fileSystem;&gt; private Configuration conf;&gt; @Before&gt; public void before() throws IOException &#123;&gt; conf=new Configuration();&gt; conf.addResource("core-site.xml");&gt; conf.addResource("hdfs-site.xml");&gt; fileSystem=FileSystem.newInstance(conf);&gt; &#125;&gt; &gt; @Test&gt; public void testConfig()&#123;&gt; String value = conf.get("dfs.replication");&gt; System.out.println(value);&gt; &#125;&gt; &gt; @Test&gt; public void testUpload01() throws IOException &#123;&gt; String file="C:\\Users\\HIAPAD\\Desktop\\SpringBoot启动原理.pdf";&gt; Path dst=new Path("/demo/access/springBoot.pdf");&gt; InputStream is = new FileInputStream(file);&gt; OutputStream os = fileSystem.create(dst, new Progressable() &#123;&gt; public void progress() &#123;&gt; System.out.print(".");&gt; &#125;&gt; &#125;);&gt; IOUtils.copyBytes(is,os,1024,true);&gt; &#125;&gt; @Test&gt; public void testUpload02() throws IOException &#123;&gt; Path src=new Path("C:\\Users\\HIAPAD\\Desktop\\SpringBoot启动原理.pdf");&gt; Path dst=new Path("/springBoot1.pdf");&gt; fileSystem.copyFromLocalFile(src,dst);&gt; &#125;&gt; &gt; @Test&gt; public void testDownload01() throws IOException &#123;&gt; String file="C:\\Users\\HIAPAD\\Desktop\\SpringBoot启动原理1.pdf";&gt; Path dst=new Path("/springBoot.pdf");&gt; OutputStream os = new FileOutputStream(file);&gt; InputStream is = fileSystem.open(dst);&gt; IOUtils.copyBytes(is,os,1024,true);&gt; &#125;&gt; @Test&gt; public void testDownload02() throws IOException &#123;&gt; Path dst=new Path("C:\\Users\\HIAPAD\\Desktop\\SpringBoot启动原理3.pdf");&gt; Path src=new Path("/springBoot1.pdf");&gt; //fileSystem.copyToLocalFile(src,dst);&gt; fileSystem.copyToLocalFile(false,src,dst,true);&gt; &#125;&gt; @Test&gt; public void testDelete() throws IOException &#123;&gt; Path src=new Path("/user");&gt; &gt; fileSystem.delete(src,true);//true 表示递归删除子文件夹&gt; &#125;&gt; &gt; @Test&gt; public void testExists() throws IOException &#123;&gt; Path src=new Path("/springBoot1.pdf");&gt; boolean exists = fileSystem.exists(src);&gt; assertTrue(exists);&gt; &#125;&gt; @Test&gt; public void testMkdir() throws IOException &#123;&gt; Path src=new Path("/demo/access");&gt; boolean exists = fileSystem.exists(src);&gt; if(!exists)&#123;&gt; fileSystem.mkdirs(src);&gt; &#125;&gt; &#125;&gt; @Test&gt; public void testListFiles() throws IOException &#123;&gt; Path src=new Path("/");&gt; RemoteIterator&lt;LocatedFileStatus&gt; files = fileSystem.listFiles(src, true);&gt; &gt; while (files.hasNext())&#123;&gt; LocatedFileStatus file = files.next();&gt; System.out.println(file.getPath()+" "+file.isFile()+" "+file.getLen());&gt; BlockLocation[] locations = file.getBlockLocations();&gt; for (BlockLocation location : locations) &#123;&gt; System.out.println("offset:"+location.getOffset()+",length:"+location.getLength());&gt; &#125;&gt; &#125;&gt; &gt; &#125;&gt; @Test&gt; public void testDeleteWithTrash() throws IOException &#123;&gt; Trash trash=new Trash(fileSystem,conf);&gt; Path dst=new Path("/springBoot1.pdf");&gt; trash.moveToTrash(dst);&gt; &#125;&gt; @After&gt; public void after() throws IOException &#123;&gt; fileSystem.close();&gt; &#125;&gt; &gt; &#125;&gt; &gt; &gt; Map ReduceMap Reduce是一种编程模型，用于大规模数据集（大于1TB）的并行运算。 概念”Map（映射）”和”Reduce（归约）”，是它们的主要思想，都是从函数式编程(数据不动代码动)语言里借来的，还有从矢量编程(分阶段对任务划分,每个阶段实现并行)语言里借来的特性。概念”Map（映射）”和”Reduce（归约）”，是它们的主要思想，都是从函数式编程语言里借来的，还有从矢量编程语言里借来的特性。 MapReduce是Hadoop的一个并行计算框架,将一个计算任务拆分成为两个阶段分别是Map阶段和Reduce阶段.Map Reduce计算框架充分利用了存储节点(datanode)所在的物理主机的计算资源(内存/CPU/网络/少许磁盘)进行并行计算.MapReduce框架会在所有的存储节点上分别启动一个Node Manager进程实现对存储节点的计算资源的管理和使用.默认情况下Node Manager会将本进程运行的物理主机的计算资源抽象成8个计算单元,每个单元称为一个Container,所有Node Manager都必须听从Resource Manager调度.Resource Manager负责计算资源的统筹分配. Map Reduce计算流程 ==Resource Manager==:统筹计算资源,管理所有NodeManager,进行资源分配 ==Node Manager==:管理物理主机上的计算资源Container,负责向RM汇报自身状态信息 ==MRAppMaster==:计算任务的Master,负责申请计算资源,协调计算任务. ==YarnChild==:负责做实际计算的任务/进程(MapTask/ReduceTask) ==Container==:是计算资源的抽象代表着一组内存/cpu/网路的占用.无论是MRAppMaster还是YarnChild运行时都需要消耗一个Container逻辑. YARN环境搭建配置文件 etc/hadoop/yarn-site.xml 123456789101112&gt; &lt;property&gt;&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&gt; &lt;/property&gt;&gt; &lt;!--Resource Manager--&gt;&gt; &lt;property&gt;&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;&gt; &lt;value&gt;CentOS&lt;/value&gt;&gt; &lt;/property&gt;&gt; &gt; &gt; etc/hadoop/mapred-site.xml 1234567&gt; &lt;property&gt;&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt;&gt; &lt;value&gt;yarn&lt;/value&gt;&gt; &lt;/property&gt;&gt; &gt; &gt; 启动计算服务 1234567891011&gt; [root@CentOS ~]# start-yarn.sh &gt; [root@CentOS ~]# jps&gt; 11459 NameNode&gt; 11575 DataNode&gt; 11722 SecondaryNameNode&gt; 18492 ResourceManager&gt; 18573 NodeManager&gt; &gt; &gt; &gt;]]></content>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据简介以及单机搭建]]></title>
    <url>%2F2020%2F01%2F13%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%AE%80%E4%BB%8B%E4%BB%A5%E5%8F%8A%E5%8D%95%E6%9C%BA%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[大数据大数据特点 5V特点：Volume（大量）、Velocity（高速）、Variety（多样）、Value（价值）Veracity（真实性） 数据量级大/数据时效性|数据处理速度快/数据多样性/数据有价值-降噪 大数据与传统的数据技术的差别 1、数据规模大：传统数据技术主要是利用现有存在关系性数据库中的数据，对这些数据进行分析、处理，找到一些关联，并利用数据关联性创造价值。这些数据的规模相对较小，可以利用数据库的分析工具处理。而大数据的数据量非常大，不可能利用数据库分析工具分析。 ​ 2、非结构化数据：传统数据主要在关系性数据库中分析，而大数据可以处理图像、声音、文件等非结构化数据。 3、处理方式不同：因为数据规模大、非结构化数据这两方面因素，导致大数据在分析时不能取全部数据做分析。大数据分析时如何选取数据?这就需要根据一些标签来抽取数据。所以大数据处理过程中，比传统数据增加了一个过程Stream。就是在写入数据的时候，在数据上打一个标签，之后在利用大数据的时候，根据标签抽取数据。 大数据面临问题? 存储:单机存储有限,如何解决海量数据存储? 分析:如何在合理时间范围内对数据完成节本运算? 分布式:通常将夸机器/跨进程/跨虚拟机架构称为分布式架构,因为硬件垂直提升成本较高且不可控,相比较垂直提升硬件水平扩展成本较低,能够使得投入和产出趋近于线性. 大数据分析方案哪些?Map Reduce:代表基于磁盘离线大数据静态批处理框架-延迟较高30分钟+ Spark:代表基于内存近实时(离线)大数据静态批处理框架-几乎是Map Reduce的10~100倍速度 Storm|Spark Streaming| Flink|Kafka Stream:实时的流(流程)处理框架,达到对记录级别的数据显示毫秒级处理. HadoopHadoop是在2006年雅虎从Nutch(给予Java爬虫框架)工程中剥离一套分布式的解决方案.该方案参考了Goggle的GFS(Google File System)和MapReduce论文,当时发布的版本称为Hadoop-1.x,并且在2010年雅虎对Hadoop做又一次升级,该次升级的目的是优化了Hadoop的MapReduce框架,使得Hadoop更加易用,用户只需要少许配置,就可以使用hadoop实现海量数据存储和大规模数据集的分析.一个由Apache基金会所开发的分布式系统基础架构。 ==HDFS==: hadoop distribute filesysterm ==Map Reduce==:hadoop中的分布式计算框架,实现对海量数据并行分析和计算. Hadoop Eco System (hadoop的生态系统圈)HDFS:分布式存储系统 mapreduce:并行计算框架 hbase:基于HDFS之上一款NoSQL数据库(名符其实海量数据存储解决方案) hive:会一款SQL的解析引擎,可以将SQL翻译成MapReduce任务,将任务提交给MapReduce框架. flume:分布式日志采集系统,用于搜集海量数据,并且存储到HDFS/Hbase. Kafka:分布式消息系统,实现分布系统间解耦和海量数据的缓冲. zookeeper:分布式协调服务,用于服务注册中心/配置中心/集群选举/状态监测/分布式锁 HDFS 环境搭建(伪分布式单机-测试) Window 安装64 bit CentOS(需要额外开启Intel 虚拟化技术) 安装JDK 12345678910一1、查询要安装jdk的版本：命令：yum -y list java*2、安装jdk命令：yum install -y java-1.8.0-openjdk.x86_643、查询jdk版本命令：java -version默认给安装到usr/lib/jvm/二、解压（常用） rpm -ivh jdk-8u171-linux-x64.rpm 安装JDK配置JAVA_HOME 1234567891011vi /etc/profile #系统变量[root@CentOS ~]# vi /root/.bashrc # 用户变量JAVA_HOME=/usr/java/latestPATH=$PATH:$JAVA_HOME/binCLASSPATH=.export JAVA_HOMEexport PATHexport CLASSPATH[root@CentOS ~]# source /root/.bashrc [root@CentOS ~]# jps1495 Jps 参考:https://blog.csdn.net/yuzongtao/article/details/44700927 尝试[root@CentOS ~]# yum install lrzsz -y组件,如果用户将JAVA_HOME配置在系统变量中/etc/profile需要在安装hadoop时候额外配置etc/hadoop/hadoop-env.sh,因此推荐配置在用户变量中. 配置主机名和IP映射关系/etc/hosts 1234[root@CentOS ~]# vi /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.169.139 CentOS 在分布式系统中很多服务都是以主机名标示节点,因此配置IP和主机名的映射关系.用户可以查看以下文件 123[root@CentOS ~]# cat /etc/sysconfig/networkNETWORKING=yesHOSTNAME=CentOS 关闭防火墙服务centos6 centOS7关闭防火墙：https://blog.csdn.net/TTTZZZTTTZZZ/article/details/81483204 1234567[root@CentOS ~]# service iptables stop #关闭服务iptables: Setting chains to policy ACCEPT: filter [ OK ]iptables: Flushing firewall rules: [ OK ]iptables: Unloading modules: [ OK ][root@CentOS ~]# chkconfig iptables off #关闭开机自起[root@CentOS ~]# chkconfig --list | grep iptables iptables 0:off 1:off 2:off 3:off 4:off 5:off 6:off 因为搭建分布式服务之间可能会产生相互的调度,为了保证正常的通信,一般需要关闭防火墙. 配置主机SSH免密码认证(密匙) SSH 为 Secure Shell 的缩写，SSH 为建立在应用层基础上的安全协议，专为远程登录会话和其他网络服务提供安全性的协议。 ==基于口令的安全验证==:基于口令用户名/密码 ==基于密匙的安全验证==: 需要依靠密匙，也就是你必须为自己创建一对密匙，并把公用密匙放在需要访问的服务器上。如果你要连接到SSH服务器上，客户端软件就会向服务器发出请求，请求用你的密匙进行安全验证。服务器收到请求之后，先在该服务器上你的主目录下寻找你的公用密匙，然后把它和你发送过来的公用密匙进行比较。如果两个密匙一致，服务器就用公用密匙加密“质询”（challenge）并把它发送给客户端软件。客户端软件收到“质询”之后就可以用你的私人密匙解密再把它发送给服务器。 123456789101112131415161718192021222324252627282930313233[root@CentOS ~]# ssh-keygen -t rsaGenerating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): Created directory '/root/.ssh'.Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:c3:b7:c4:e3:5e:6f:db:69:48:23:1e:f7:81:9b:d1:8e root@CentOSThe key's randomart image is:+--[ RSA 2048]----+| || || || . . || S = o || = = * o || + * X .|| . o E.=.|| . .+o.|+-----------------+[root@CentOS ~]# ssh-copy-id CentOSThe authenticity of host 'centos (192.168.169.139)' can't be established.RSA key fingerprint is f0:63:ed:d6:21:3b:b5:47:ad:e2:7f:98:bd:8f:54:94.Are you sure you want to continue connecting (yes/no)? `yes`Warning: Permanently added 'centos,192.168.169.139' (RSA) to the list of known hosts.root@centos's password:`****` Now try logging into the machine, with "ssh 'CentOS'", and check in: .ssh/authorized_keysto make sure we haven't added extra keys that you weren't expecting. HADOOP HDFS安装与配置 参考:http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html 解压并且配置HADOOP_HOME 12345678910111213[root@CentOS ~]# tar -zxf hadoop-2.6.0_x64.tar.gz -C /usr/HADOOP_HOME=/usr/hadoop-2.6.0JAVA_HOME=/usr/java/latestPATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbinCLASSPATH=.export JAVA_HOMEexport PATHexport CLASSPATHexport HADOOP_HOME[root@CentOS ~]# source /root/.bashrc [root@CentOS ~]# echo $HADOOP_HOME HADOOP_HOME环境变量被第三方产品所依赖例如:hbase/hive/flume/Spark在集成Hadoop的时候,是通过读取HADOOP_HOME环境变量确定HADOOP位置. 配置etc/hadoop/core-site.xml 123456789&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://CentOS:9000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/hadoop-2.6.0/hadoop-$&#123;user.name&#125;&lt;/value&gt;&lt;/property&gt; 配置etc/hadoop/hdfs-site.xml 1234&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; 配置etc/hadoop/slaves 1CentOS 启动HDFS 1.如果是第一次启动HDFS,需要格式化namenode 12345[root@CentOS ~]# hdfs namenode -format...19/01/02 20:19:37 INFO common.Storage: Storage directory /usr/hadoop-2.6.0/hadoop-root/dfs/name has been successfully formatted....再次启动需要删除/usr/hadoop-2.6.0/hadoop-root/ 格式化成功后,用户可以看到以下目录结构 123456789101.安装插件 yum install -y tree[root@CentOS ~]# tree /usr/hadoop-2.6.0/hadoop-root//usr/hadoop-2.6.0/hadoop-root/└── dfs └── name └── current ├── fsimage_0000000000000000000 ├── fsimage_0000000000000000000.md5 ├── seen_txid └── VERSION 2.启动HDFS服务 123456789101112131415161718192021[root@CentOS ~]# start-dfs.sh Starting namenodes on [CentOS]CentOS: namenode running as process 1846. Stop it first.CentOS: starting datanode, logging to /usr/hadoop-2.6.0/logs/hadoop-root-datanode-CentOS.outStarting secondary namenodes [0.0.0.0]The authenticity of host '0.0.0.0 (0.0.0.0)' can't be established.RSA key fingerprint is f0:63:ed:d6:21:3b:b5:47:ad:e2:7f:98:bd:8f:54:94.Are you sure you want to continue connecting (yes/no)? yes0.0.0.0: Warning: Permanently added '0.0.0.0' (RSA) to the list of known hosts.0.0.0.0: starting secondarynamenode, logging to /usr/hadoop-2.6.0/logs/hadoop-root-secondarynamenode-CentOS.out[root@CentOS ~]# jps2370 Jps2133 DataNode1846 NameNode2267 SecondaryNameNode[root@CentOS ~]# stop-dfs.sh Stopping namenodes on [CentOS]CentOS: stopping namenodeCentOS: stopping datanodeStopping secondary namenodes [0.0.0.0]0.0.0.0: stopping secondarynamenode 或者用户可以访问浏览器:http://192.168.169.139:50070 https://www.cnblogs.com/zyanrong/p/11774997.html 1234[root@CentOS ~]# hdfs dfs -put /root/jdk-8u171-linux-x64.rpm /[root@CentOS ~]# hdfs dfs -ls /Found 1 items-rw-r--r-- 1 root supergroup 175262413 2019-01-02 20:29 /jdk-8u171-linux-x64.rpm]]></content>
      <tags>
        <tag>bigdata</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NoSql相关知识]]></title>
    <url>%2F2020%2F01%2F13%2FNoSql%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[NoSQL 简介NoSQL(NoSQL = Not Only SQL )，意即”不仅仅是SQL”。 在现代的计算系统上每天网络上都会产生庞大的数据量。 这些数据有很大一部分是由关系数据库管理系统（RDBMS）来处理。 1970年 E.F.Codd’s提出的关系模型的论文 “A relational model of data for large shared data banks”，这使得数据建模和应用程序编程更加简单。 通过应用实践证明，关系模型是非常适合于客户服务器编程，远远超出预期的利益，今天它是结构化数据存储在网络和商务应用的主导技术。 NoSQL 是一项全新的数据库革命性运动，早期就有人提出，发展至2009年趋势越发高涨。NoSQL的拥护者们提倡运用非关系型的数据存储，相对于铺天盖地的关系型数据库运用，这一概念无疑是一种全新的思维的注入。 关系型数据库遵循ACID规则事务在英文中是transaction，和现实世界中的交易很类似，它有如下四个特性： 1、A (Atomicity) 原子性 原子性很容易理解，也就是说事务里的所有操作要么全部做完，要么都不做，事务成功的条件是事务里的所有操作都成功，只要有一个操作失败，整个事务就失败，需要回滚。 比如银行转账，从A账户转100元至B账户，分为两个步骤：1）从A账户取100元；2）存入100元至B账户。这两步要么一起完成，要么一起不完成，如果只完成第一步，第二步失败，钱会莫名其妙少了100元。 2、C (Consistency) 一致性 一致性也比较容易理解，也就是说数据库要一直处于一致的状态，事务的运行不会改变数据库原本的一致性约束。 例如现有完整性约束a+b=10，如果一个事务改变了a，那么必须得改变b，使得事务结束后依然满足a+b=10，否则事务失败。 3、I (Isolation) 独立性 所谓的独立性是指并发的事务之间不会互相影响，如果一个事务要访问的数据正在被另外一个事务修改，只要另外一个事务未提交，它所访问的数据就不受未提交事务的影响。 比如现在有个交易是从A账户转100元至B账户，在这个交易还未完成的情况下，如果此时B查询自己的账户，是看不到新增加的100元的。 4、D (Durability) 持久性 持久性是指一旦事务提交后，它所做的修改将会永久的保存在数据库上，即使出现宕机也不会丢失。 分布式系统分布式系统（distributed system）由多台计算机和通信的软件组件通过计算机网络连接（本地网络或广域网）组成。 分布式系统是建立在网络之上的软件系统。正是因为软件的特性，所以分布式系统具有高度的内聚性和透明性。 因此，网络和分布式系统之间的区别更多的在于高层软件（特别是操作系统），而不是硬件。 分布式系统可以应用在不同的平台上如：Pc、工作站、局域网和广域网上等。 分布式计算的优点可靠性（容错） ： 分布式计算系统中的一个重要的优点是可靠性。一台服务器的系统崩溃并不影响到其余的服务器。 可扩展性： 在分布式计算系统可以根据需要增加更多的机器。 资源共享： 共享数据是必不可少的应用，如银行，预订系统。 灵活性： 由于该系统是非常灵活的，它很容易安装，实施和调试新的服务。 更快的速度： 分布式计算系统可以有多台计算机的计算能力，使得它比其他系统有更快的处理速度。 开放系统： 由于它是开放的系统，本地或者远程都可以访问到该服务。 更高的性能： 相较于集中式计算机网络集群可以提供更高的性能（及更好的性价比）。 分布式计算的缺点故障排除： 故障排除和诊断问题。 软件： 更少的软件支持是分布式计算系统的主要缺点。 网络： 网络基础设施的问题，包括：传输问题，高负载，信息丢失等。 安全性： 开放系统的特性让分布式计算系统存在着数据的安全性和共享的风险等问题。 什么是NoSQL?NoSQL，指的是非关系型的数据库。NoSQL有时也称作Not Only SQL的缩写，是对不同于传统的关系型数据库的数据库管理系统的统称。 NoSQL用于超大规模数据的存储。（例如谷歌或Facebook每天为他们的用户收集万亿比特的数据）。这些类型的数据存储不需要固定的模式，无需多余操作就可以横向扩展。 为什么使用NoSQL ?今天我们可以通过第三方平台（如：Google,Facebook等）可以很容易的访问和抓取数据。用户的个人信息，社交网络，地理位置，用户生成的数据和用户操作日志已经成倍的增加。我们如果要对这些用户数据进行挖掘，那SQL数据库已经不适合这些应用了, NoSQL 数据库的发展却能很好的处理这些大的数据。 实例社会化关系网: Each record: UserID1, UserID2Separate records: UserID, first_name,last_name, age, gender,…Task: Find all friends of friends of friends of … friends of a given user. Wikipedia 页面 : Large collection of documentsCombination of structured and unstructured dataTask: Retrieve all pages regarding athletics of Summer Olympic before 1950. RDBMS vs NoSQLRDBMS 高度组织化结构化数据 结构化查询语言（SQL） (SQL) 数据和关系都存储在单独的表中。 数据操纵语言，数据定义语言 严格的一致性 基础事务 NoSQL 代表着不仅仅是SQL 没有声明性查询语言 没有预定义的模式-键 - 值对存储，列存储，文档存储，图形数据库 最终一致性，而非ACID属性 非结构化和不可预知的数据 CAP定理 高性能，高可用性和可伸缩性 NoSQL 简史NoSQL一词最早出现于1998年，是Carlo Strozzi开发的一个轻量、开源、不提供SQL功能的关系数据库。 2009年，Last.fm的Johan Oskarsson发起了一次关于分布式开源数据库的讨论[2]，来自Rackspace的Eric Evans再次提出了NoSQL的概念，这时的NoSQL主要指非关系型、分布式、不提供ACID的数据库设计模式。 2009年在亚特兰大举行的”no:sql(east)”讨论会是一个里程碑，其口号是”select fun, profit from real_world where relational=false;”。因此，对NoSQL最普遍的解释是”非关联型的”，强调Key-Value Stores和文档数据库的优点，而不是单纯的反对RDBMS。 CAP定理（CAP theorem）在计算机科学中, CAP定理（CAP theorem）, 又被称作 布鲁尔定理（Brewer’s theorem）, 它指出对于一个分布式计算系统来说，不可能同时满足以下三点: 一致性(Consistency) (所有节点在同一时间具有相同的数据) 可用性(Availability) (保证每个请求不管成功或者失败都有响应) 分隔容忍(Partition tolerance) (系统中任意信息的丢失或失败不会影响系统的继续运作) CAP理论的核心是：一个分布式系统不可能同时很好的满足一致性，可用性和分区容错性这三个需求，最多只能同时较好的满足两个。 因此，根据 CAP 原理将 NoSQL 数据库分成了满足 CA 原则、满足 CP 原则和满足 AP 原则三 大类： CA - 单点集群，满足一致性，可用性的系统，通常在可扩展性上不太强大。 CP - 满足一致性，分区容忍性的系统，通常性能不是特别高。 AP - 满足可用性，分区容忍性的系统，通常可能对一致性要求低一些。 NoSQL的优点/缺点优点: - 高可扩展性 - 分布式计算 - 低成本 - 架构的灵活性，半结构化数据 - 没有复杂的关系 缺点: - 没有标准化 - 有限的查询功能（到目前为止） - 最终一致是不直观的程序 BASEBASE：Basically Available, Soft-state, Eventually Consistent。 由 Eric Brewer 定义。 CAP理论的核心是：一个分布式系统不可能同时很好的满足一致性，可用性和分区容错性这三个需求，最多只能同时较好的满足两个。 BASE是NoSQL数据库通常对可用性及一致性的弱要求原则: Basically Availble –基本可用 Soft-state –软状态/柔性事务。 “Soft state” 可以理解为”无连接”的, 而 “Hard state” 是”面向连接”的 Eventual Consistency – 最终一致性， 也是是 ACID 的最终目的。 ACID vs BASE ACID BASE 原子性(Atomicity) 基本可用(Basically Available) 一致性(Consistency) 软状态/柔性事务(Soft state) 隔离性(Isolation) 最终一致性 (Eventual consistency) 持久性 (Durable) NoSQL 数据库分类 类型 部分代表 特点 列存储 Hbase Cassandra Hypertable 顾名思义，是按列存储数据的。最大的特点是方便存储结构化和半结构化数据，方便做数据压缩，对针对某一列或者某几列的查询有非常大的IO优势。 文档存储 MongoDB CouchDB 文档存储一般用类似json的格式存储，存储的内容是文档型的。这样也就有机会对某些字段建立索引，实现关系数据库的某些功能。 key-value存储 Tokyo Cabinet / TyrantBerkeley DBMemcacheDBRedis 可以通过key快速查询到其value。一般来说，存储不管value的格式，照单全收。（Redis包含了其他功能） 图形存储 Neo4JFlockDB 图形关系的最佳存储。使用传统关系数据库来解决的话性能低下，而且设计使用不方便。 对象存储 db4oVersant 通过类似面向对象语言的语法操作数据库，通过对象的方式存取数据。 xml数据库 Berkeley DB XMLBaseX 高效的存储XML数据，并支持XML的内部查询语法，比如XQuery,Xpath。 不同分类特点对比 分类 Examples举例 典型应用场景 数据模型 优点 缺点 键值（key-value） Tokyo Cabinet/Tyrant， Redis， Voldemort， Oracle BDB 内容缓存，主要用于处理大量数据的高访问负载，也用于一些日志系统等等。 Key 指向 Value 的键值对，通常用hash table来实现 查找速度快 数据无结构化，通常只被当作字符串或者二进制数据 列存储数据库 Cassandra， HBase， Riak 分布式的文件系统 以列簇式存储，将同一列数据存在一起 查找速度快，可扩展性强，更容易进行分布式扩展 功能相对局限 文档型数据库 CouchDB， MongoDb Web应用（与Key-Value类似，Value是结构化的，不同的是数据库能够了解Value的内容） Key-Value对应的键值对，Value为结构化数据 数据结构要求不严格，表结构可变，不需要像关系型数据库一样需要预先定义表结构 查询性能不高，而且缺乏统一的查询语法。 图形(Graph)数据库 Neo4J， InfoGrid， Infinite Graph 社交网络，推荐系统等。专注于构建关系图谱 图结构 利用图结构相关算法。比如最短路径寻址，N度关系查找等 很多时候需要对整个图做计算才能得出需要的信息，而且这种结构不太好做分布式的集群方案。]]></content>
      <tags>
        <tag>NoSql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins+svn+maven持续集成]]></title>
    <url>%2F2020%2F01%2F10%2Fjenkins-svn-maven%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%2F</url>
    <content type="text"><![CDATA[jenkins+svn+maven持续集成一.安装svn1yum install subversion 配置2.1. 创建仓库我们这里在/home下建立一个名为svn的仓库（repository），以后所有代码都放在这个下面，创建成功后在svn下面多了几个文件夹。 12345[root@localhost /]# cd /home[root@localhost home]# mkdir svn[root@localhost home]# svnadmin create /home/svn[root@localhost home]# ls svnconf db format hooks locks README.txt 我们这里特别关注一下conf文件夹，这个是存放配置文件的 123[root@localhost home]# cd svn/conf[root@localhost conf]# lsauthz passwd svnserve.conf 其中： authz 是权限控制文件 passwd 是帐号密码文件 svnserve.conf 是SVN服务配置文件 接下来我们依次修改这3个文件。 2.2. 配置passwd1234[root@localhost conf]# vi passwd [users]test1=123456test2=123456 上面的例子中我们创建了2个用户，一个test1，一个test2 2.3. 配置authz123456[root@localhost conf]# vi authz [/]liuxianan=rwtest1=rtest2=r*= 上面配置的含义是，liuxianan对/home/svn/下所有文件具有可读可写权限，test只有只读权限，除此之外，其它用户均无任何权限，最后一行*=很重要不能少。 2.3.1. 拓展：使用用户分组这个我一般不用，但是记录下来。 还是这个文件： 12345678[root@localhost conf]# vi authz[groups]group1 = liuxianangroup2 = test1,test2[/]@group1 = rw@group2 = r* = 上面配置中创建了2个分组，分组1的用户可读可写，分组2的用户只读。 格式说明： 版本库目录格式：[&lt;版本库&gt;:/项目/目录]@&lt;用户组名&gt; = &lt;权限&gt;&lt;用户名&gt; = &lt;权限&gt; 2.4. 配置svnserve.conf1234567[root@localhost conf]# vi svnserve.conf 打开下面的5个注释anon-access = read #匿名用户可读auth-access = write #授权用户可写password-db = passwd #使用哪个文件作为账号文件authz-db = authz #使用哪个文件作为权限文件realm = /home/svn # 认证空间名，版本库所在目录 2点注意： 最后一行的realm记得改成你的svn目录 打开注释时切记前面不要留有空格，否则可能有问题（网上说的，我没有亲测） 启动与停止12[root@localhost conf]# svnserve -d -r /home/svn（启动）[root@localhost conf]#killall svnserve（停止） 上述启动命令中，-d表示守护进程， -r 表示在后台执行。停止还可以采用杀死进程的方式： 1234[root@localhost conf]# ps -ef|grep svnserveroot 4908 1 0 21:32 ? 00:00:00 svnserve -d -r /home/svnroot 4949 4822 0 22:05 pts/0 00:00:00 grep svnserve[root@localhost conf]# kill -9 4908 最后来个总的截图： 客户端连接这里使用TortoiseSVN，输入地址svn://你的IP 即可，不出意外输入用户名和密码就能连接成功了。 默认端口3690，如果你修改了端口，那么要记得加上端口号。 二、安装maven12345678910111.进入maven官网下载文件，解压maven的tar文件tar -zxvf apache-maven-XXX-bin.tar.gz 2.编辑配置文件vim /etc/profile3.添加如下配置export MAVEN_HOME=/opt/maven/apache-maven-3.6.3export PATH=$MAVEN_HOME/bin:$PATH4.刷新配置文件source /etc/profile5.检测mvn -v 三、安装Jenkins1.通过Tomcat将Jenkins.war放入webapps中 访问： http://ip:port/jenkins Jenkins安装完成后配置参考：https://blog.csdn.net/baidu_38432732/article/details/80499898]]></content>
      <tags>
        <tag>Jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker]]></title>
    <url>%2F2019%2F12%2F09%2Fdocker%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[docker安装与使用1.安装centOS7 安装gcc [root@localhost ~]# yum -y install gcc 安装gcc-c++ [root@localhost ~]# yum -y install gcc-c++验证gcc是否安装成功[root@localhost ~]#gcc -v————————————————https://blog.csdn.net/li1325169021/article/details/90780627 如果已安装docker，需要先卸载旧版本 12345678yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-engine 安装所需要的软件包 123yum install -y yum-utils \ device-mapper-persistent-data \ lvm2 添加软件源信息1yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 更新并安装 Docker-CE1234更新yum软件包索引yum makecache fast安装 yum -y install docker-ce 开启Docker服务12service docker startsystemctl start docker 2.Docker hello world使用docker run 在容器内启动一个应用程序 1docker run ubuntu:15.10 /bin/echo "Hello world" docker: Docker 的二进制执行文件。 run: 与前面的 docker 组合来运行一个容器。 ubuntu:15.10 指定要运行的镜像，Docker 首先从本地主机上查找镜像是否存在，如果不存在，Docker 就会从镜像仓库 Docker Hub 下载公共镜像。 /bin/echo “Hello world”: 在启动的容器里执行的命令 运行交互式容器 12345678docker run -i -t ubuntu:15.10 /bin/bash-t:在新容器内指定一个终端或伪终端-i:允许对容器内的标准输入进行交互/bin/bash：放在镜像名后的是命令，这里我们希望有个交互式 Shell，因此用的是 /bin/bash。出现 '：/#' 表示进入 ubuntu:15.10 容器cat /proc/version:查看当前系统的版本信息ls 查看文件exit /ctrl+D 退出 启动容器（后台模式）使用以下命令创建一个以进程方式运行的容器 12runoob@runoob:~$ docker run -d ubuntu:15.10 /bin/sh -c &quot;while true; do echo hello world; sleep 1; done&quot;2b1b7a428627c51ab8810d541d759f072b4fc75487eed05812646b8534a2fe63 在输出中，我们没有看到期望的 “hello world”，而是一串长字符 2b1b7a428627c51ab8810d541d759f072b4fc75487eed05812646b8534a2fe63 这个长字符串叫做容器 ID，对每个容器来说都是唯一的，我们可以通过容器 ID 来查看对应的容器发生了什么。 首先，我们需要确认容器有在运行，可以通过 docker ps 来查看： 123runoob@runoob:~$ docker psCONTAINER ID IMAGE COMMAND ... 5917eac21c36 ubuntu:15.10 &quot;/bin/sh -c &apos;while t…&quot; ... 输出详情介绍： CONTAINER ID: 容器 ID。 IMAGE: 使用的镜像。 COMMAND: 启动容器时运行的命令。 CREATED: 容器的创建时间。 STATUS: 容器状态。 状态有7种： created（已创建） restarting（重启中） running（运行中） removing（迁移中） paused（暂停） exited（停止） dead（死亡） PORTS: 容器的端口信息和使用的连接类型（tcp\udp）。 NAMES: 自动分配的容器名称。 常用命令：123456789101112131415161718192021222324 在容器内使用 docker logs 命令，查看容器内的标准输出 docker logs +IDdocker logs +容器名称使用ubuntu镜像启动一个容器$ docker run -it ubuntu /bin/bash查看所有容器docker ps -a启动一个已停止的容器docker start +id后台运行在大部分的场景下，我们希望 docker 的服务是在后台运行的，我们可以过 -d 指定容器的运行模式。$ docker run -itd --name ubuntu-test ubuntu /bin/bash停止docker stop +Iddocker stop +name重启docker restart+id后台启动时进入容器：docker attach使用：docker attach +iddocker exec 使用 docker exec 命令，因为此退出容器终端，不会导致容器的停止。docker exec -it +id /bin/bash 导出和导入容器导出容器 如果要导出本地某个容器，可以使用 docker export 命令。 1$ docker export 1e560fca3906 &gt; ubuntu.tar 导出容器 1e560fca3906 快照到本地文件 ubuntu.tar。 这样将导出容器快照到本地文件。 导入容器快照 可以使用 docker import 从容器快照文件中再导入为镜像，以下实例将快照文件 ubuntu.tar 导入到镜像 test/ubuntu:v1: 1$ cat docker/ubuntu.tar | docker import - test/ubuntu:v1 此外，也可以通过指定 URL 或者某个目录来导入，例如： 1$ docker import http://example.com/exampleimage.tgz example/imagerepo 删除容器删除容器使用 docker rm 命令： 1$ docker rm -f 1e560fca3906 下面的命令可以清理掉所有处于终止状态的容器。 $ docker container prune 运行一个 web 应用前面我们运行的容器并没有一些什么特别的用处。 接下来让我们尝试使用 docker 构建一个 web 应用程序。 我们将在docker容器中运行一个 Python Flask 应用来运行一个web应用。 12runoob@runoob:~# docker pull training/webapp # 载入镜像runoob@runoob:~# docker run -d -P training/webapp python app.py 参数说明: -d:让容器在后台运行。 -P:将容器内部使用的网络端口映射到我们使用的主机上。 查看 WEB 应用容器使用 docker ps 来查看我们正在运行的容器： 123runoob@runoob:~# docker psCONTAINER ID IMAGE COMMAND ... PORTS d3d5e39ed9d3 training/webapp &quot;python app.py&quot; ... 0.0.0.0:32769-&gt;5000/tcp 这里多了端口信息。 12PORTS0.0.0.0:32769-&gt;5000/tcp Docker 开放了 5000 端口（默认 Python Flask 端口）映射到主机端口 32769 上。 这时我们可以通过浏览器访问WEB应用 我们也可以通过 -p 参数来设置不一样的端口： 1runoob@runoob:~$ docker run -d -p 5000:5000 training/webapp python app.py docker ps查看正在运行的容器 1234runoob@runoob:~# docker psCONTAINER ID IMAGE PORTS NAMESbf08b7f2cd89 training/webapp ... 0.0.0.0:5000-&gt;5000/tcp wizardly_chandrasekhard3d5e39ed9d3 training/webapp ... 0.0.0.0:32769-&gt;5000/tcp xenodochial_hoov 容器内部的 5000 端口映射到我们本地主机的 5000 端口上。 网络端口的快捷方式通过 docker ps 命令可以查看到容器的端口映射，docker 还提供了另一个快捷方式 docker port，使用 docker port 可以查看指定 （ID 或者名字）容器的某个确定端口映射到宿主机的端口号。 上面我们创建的 web 应用容器 ID 为 bf08b7f2cd89 名字为 wizardly_chandrasekhar。 我可以使用 docker port bf08b7f2cd89 或 docker port wizardly_chandrasekhar 来查看容器端口的映射情况。 1234runoob@runoob:~$ docker port bf08b7f2cd895000/tcp -&gt; 0.0.0.0:5000runoob@runoob:~$ docker port wizardly_chandrasekhar5000/tcp -&gt; 0.0.0.0:5000 查看 WEB 应用程序日志docker logs [ID或者名字] 可以查看容器内部的标准输出。 1234runoob@runoob:~$ docker logs -f bf08b7f2cd89 * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)192.168.239.1 - - [09/May/2016 16:30:37] &quot;GET / HTTP/1.1&quot; 200 -192.168.239.1 - - [09/May/2016 16:30:37] &quot;GET /favicon.ico HTTP/1.1&quot; 404 - -f: 让 docker logs 像使用 tail -f 一样来输出容器内部的标准输出。 从上面，我们可以看到应用程序使用的是 5000 端口并且能够查看到应用程序的访问日志。 查看WEB应用程序容器的进程我们还可以使用 docker top 来查看容器内部运行的进程 123runoob@runoob:~$ docker top wizardly_chandrasekharUID PID PPID ... TIME CMDroot 23245 23228 ... 00:00:00 python app.py 检查 WEB 应用程序使用 docker inspect 来查看 Docker 的底层信息。它会返回一个 JSON 文件记录着 Docker 容器的配置和状态信息。 1234567891011121314151617181920212223runoob@runoob:~$ docker inspect wizardly_chandrasekhar[ &#123; &quot;Id&quot;: &quot;bf08b7f2cd897b5964943134aa6d373e355c286db9b9885b1f60b6e8f82b2b85&quot;, &quot;Created&quot;: &quot;2018-09-17T01:41:26.174228707Z&quot;, &quot;Path&quot;: &quot;python&quot;, &quot;Args&quot;: [ &quot;app.py&quot; ], &quot;State&quot;: &#123; &quot;Status&quot;: &quot;running&quot;, &quot;Running&quot;: true, &quot;Paused&quot;: false, &quot;Restarting&quot;: false, &quot;OOMKilled&quot;: false, &quot;Dead&quot;: false, &quot;Pid&quot;: 23245, &quot;ExitCode&quot;: 0, &quot;Error&quot;: &quot;&quot;, &quot;StartedAt&quot;: &quot;2018-09-17T01:41:26.494185806Z&quot;, &quot;FinishedAt&quot;: &quot;0001-01-01T00:00:00Z&quot; &#125;,...... 停止 WEB 应用容器12runoob@runoob:~$ docker stop wizardly_chandrasekhar wizardly_chandrasekhar 重启WEB应用容器已经停止的容器，我们可以使用命令 docker start 来启动。 12runoob@runoob:~$ docker start wizardly_chandrasekharwizardly_chandrasekhar docker ps -l 查询最后一次创建的容器： 123# docker ps -l CONTAINER ID IMAGE PORTS NAMESbf08b7f2cd89 training/webapp ... 0.0.0.0:5000-&gt;5000/tcp wizardly_chandrasekhar 正在运行的容器，我们可以使用 docker restart 命令来重启。 移除WEB应用容器我们可以使用 docker rm 命令来删除不需要的容器 12runoob@runoob:~$ docker rm wizardly_chandrasekhar wizardly_chandrasekhar 删除容器时，容器必须是停止状态，否则会报如下错误 12runoob@runoob:~$ docker rm wizardly_chandrasekharError response from daemon: You cannot remove a running container bf08b7f2cd897b5964943134aa6d373e355c286db9b9885b1f60b6e8f82b2b85. Stop the container before attempting removal or force remove]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker]]></title>
    <url>%2F2019%2F12%2F09%2Fdocker%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[docker概念docker简介Docker 是一个开源的应用容器引擎，基于 Go 语言 并遵从Apache2.0协议开源。 Docker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 Linux 或windows机器上，也可以实现虚拟化。 容器是完全使用沙箱机制，相互之间不会有任何接口（类似 iPhone 的 app）,更重要的是容器性能开销极低。 Docker 从 17.03 版本之后分为 CE（Community Edition: 社区版） 和 EE（Enterprise Edition: 企业版） Docker的应用场景 Web 应用的自动化打包和发布。 自动化测试和持续集成、发布。 在服务型环境中部署和调整数据库或其他的后台应用。 从头编译或者扩展现有的 OpenShift 或 Cloud Foundry 平台来搭建自己的 PaaS 环境。 Docker 的优点 1、简化程序：Docker 让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，便可以实现虚拟化。Docker改变了虚拟化的方式，使开发者可以直接将自己的成果放入Docker中进行管理。方便快捷已经是 Docker的最大优势，过去需要用数天乃至数周的任务，在Docker容器的处理下，只需要数秒就能完成。 2、避免选择恐惧症：如果你有选择恐惧症，还是资深患者。那么你可以使用 Docker 打包你的纠结！比如 Docker 镜像；Docker 镜像中包含了运行环境和配置，所以 Docker 可以简化部署多种应用实例工作。比如 Web 应用、后台应用、数据库应用、大数据应用比如 Hadoop 集群、消息队列等等都可以打包成一个镜像部署。 3、节省开支：一方面，云计算时代到来，使开发者不必为了追求效果而配置高额的硬件，Docker 改变了高性能必然高价格的思维定势。Docker 与云的结合，让云空间得到更充分的利用。不仅解决了硬件管理的问题，也改变了虚拟化的方式。 相关链接Docker 官网：http://www.docker.com Github Docker 源码：https://github.com/docker/docker docker 相关介绍：https://baike.baidu.com/item/Docker/13344470?fr=aladdin Docker的组成部分 Docker镜像（Images）：Docker镜像是用于创建Docker容器的模板 Docker容器（Container）：容器是独立运行的一个或一组应用 Docker客户端（Client）：Docker客户端通过命令行或者其他工具使用Docker API与Docker守护进程通信 Docker主机（Host）：一个物理或者虚拟的机器用于执行Docker守护进程和容器 Docker仓库（Registry）：Docker仓库用来保存镜像，可以理解为代码控制中的代码仓库。Docker Hub提供了庞大的镜像集合供使用。 Docker Machine：Docker Machine是一个简化Docker安装的命令行工具，通过一个简单的命令即可在相应的平台上安装Docker，比如VirturalBox、Digital Ocean、Microsoft Azure。 Docker和虚拟机比较容器是在操作系统层面上实现虚拟化，直接复用本地主机的操作系统层面上实现虚拟化，而传统的方式则是在硬件层面上实现的。 Docker 属于 Linux 容器的一种封装，提供简单易用的容器使用接口。它是目前最流行的 Linux 容器解决方案。 而 Linux 容器是 Linux 发展出了另一种虚拟化技术，简单来讲， Linux 容器不是模拟一个完整的操作系统，而是对进程进行隔离，相当于是在正常进程的外面套了一个保护层。对于容器里面的进程来说，它接触到的各种资源都是虚拟的，从而实现与底层系统的隔离。 Docker 将应用程序与该程序的依赖，打包在一个文件里面。运行这个文件，就会生成一个虚拟容器。程序在这个虚拟容器里运行，就好像在真实的物理机上运行一样。有了 Docker ，就不用担心环境问题。 总体来说， Docker 的接口相当简单，用户可以方便地创建和使用容器，把自己的应用放入容器。容器还可以进行版本管理、复制、分享、修改，就像管理普通的代码一样。、 Docker相比于传统虚拟化方式具有更多的优势： docker 启动快速属于秒级别。虚拟机通常需要几分钟去启动 docker 需要的资源更少， docker 在操作系统级别进行虚拟化， docker 容器和内核交互，几乎没有性能损耗，性能优于通过 Hypervisor 层与内核层的虚拟化 docker 更轻量， docker 的架构可以共用一个内核与共享应用程序库，所占内存极小。同样的硬件环境， Docker 运行的镜像数远多于虚拟机数量，对系统的利用率非常高 与虚拟机相比， docker 隔离性更弱， docker 属于进程之间的隔离，虚拟机可实现系统级别隔离 安全性： docker 的安全性也更弱。 Docker 的租户 root 和宿主机 root 等同，一旦容器内的用户从普通用户权限提升为root权限，它就直接具备了宿主机的root权限，进而可进行无限制的操作。虚拟机租户 root 权限和宿主机的 root 虚拟机权限是分离的，并且虚拟机利用如 Intel 的 VT-d 和 VT-x 的 ring-1 硬件隔离技术，这种隔离技术可以防止虚拟机突破和彼此交互，而容器至今还没有任何形式的硬件隔离，这使得容器容易受到攻击 可管理性： docker 的集中化管理工具还不算成熟。各种虚拟化技术都有成熟的管理工具，例如 VMware vCenter 提供完备的虚拟机管理能力 高可用和可恢复性： docker 对业务的高可用支持是通过快速重新部署实现的。虚拟化具备负载均衡，高可用，容错，迁移和数据保护等经过生产实践检验的成熟保障机制， VMware 可承诺虚拟机 99.999% 高可用，保证业务连续性 快速创建、删除：虚拟化创建是分钟级别的， Docker 容器创建是秒级别的， Docker 的快速迭代性，决定了无论是开发、测试、部署都可以节约大量时间 交付、部署：虚拟机可以通过镜像实现环境交付的一致性，但镜像分发无法体系化。 Docker 在 Dockerfile 中记录了容器构建过程，可在集群中实现快速分发和快速部署 我们可以从下面这张表格很清楚地看到容器相比于传统虚拟机的特性的优势所在： 特性 容器 虚拟机 启动 秒级 分钟级 硬盘使用 一般为MB 一般为GB 性能 接近原生 弱于 系统支持量 单机支持上千个容器 一般是几十个 Docker的三个基本概念 从上图我们可以看到，Docker 中包括三个基本的概念： Image(镜像) Container(容器) Repository(仓库) 镜像是 Docker 运行容器的前提，仓库是存放镜像的场所，可见镜像更是 Docker 的核心。 Image (镜像) 那么镜像到底是什么呢？ Docker 镜像可以看作是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。镜像不包含任何动态数据，其内容在构建之后也不会被改变。 镜像（Image）就是一堆只读层（read-only layer）的统一视角，也许这个定义有些难以理解，下面的这张图能够帮助读者理解镜像的定义。 从左边我们看到了多个只读层，它们重叠在一起。除了最下面一层，其它层都会有一个指针指向下一层。这些层是Docker 内部的实现细节，并且能够在主机的文件系统上访问到。统一文件系统 (union file system) 技术能够将不同的层整合成一个文件系统，为这些层提供了一个统一的视角，这样就隐藏了多层的存在，在用户的角度看来，只存在一个文件系统。我们可以在图片的右边看到这个视角的形式。 Container (容器) 容器 (container) 的定义和镜像 (image) 几乎一模一样，也是一堆层的统一视角，唯一区别在于容器的最上面那一层是可读可写的。 由于容器的定义并没有提及是否要运行容器，所以实际上，容器 = 镜像 + 读写层。 Docker镜像与容器镜像是构建Docker的基石。用户基于镜像来运行自己的容器，镜像也是Docker生命周期中的“构建部分”。镜像是基于联合文件系统的一种层式结构，由一系列指令一步一步构建出来。其次Docker可以帮你构建和部署容器，你只需要把自己的应用程序或者服务打包放进容器即可。容器是基于镜像启动起来的，容器中可以运行一个或多个进程。 Repository (仓库) Docker 仓库是集中存放镜像文件的场所。镜像构建完成后，可以很容易的在当前宿主上运行，但是， 如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry (仓库注册服务器)就是这样的服务。有时候会把仓库 (Repository) 和仓库注册服务器 (Registry) 混为一谈，并不严格区分。Docker 仓库的概念跟 Git 类似，注册服务器可以理解为 GitHub 这样的托管服务。实际上，一个 Docker Registry 中可以包含多个仓库 (Repository) ，每个仓库可以包含多个标签 (Tag)，每个标签对应着一个镜像。所以说，镜像仓库是 Docker 用来集中存放镜像文件的地方类似于我们之前常用的代码仓库。 通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本 。我们可以通过&lt;仓库名&gt;:&lt;标签&gt;的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 latest 作为默认标签.。 仓库又可以分为两种形式： public(公有仓库) private(私有仓库) Docker Registry 公有仓库是开放给用户使用、允许用户管理镜像的 Registry 服务。一般这类公开服务允许用户免费上传、下载公开的镜像，并可能提供收费服务供用户管理私有镜像。 除了使用公开服务外，用户还可以在本地搭建私有 Docker Registry 。Docker 官方提供了 Docker Registry镜像，可以直接使用做为私有 Registry 服务。当用户创建了自己的镜像之后就可以使用 push 命令将它上传到公有或者私有仓库，这样下次在另外一台机器上使用这个镜像时候，只需要从仓库上 pull 下来就可以了。 我们主要把 Docker 的一些常见概念如 Image ， Container ， Repository 做了详细的阐述，也从传统虚拟化方式的角度阐述了 docker 的优势，我们从下图可以直观地看到 Docker 的架构： Docker 使用 C/S 结构，即客户端/服务器体系结构。 Docker 客户端与 Docker 服务器进行交互，Docker服务端负责构建、运行和分发 Docker 镜像。 Docker 客户端和服务端可以运行在一台机器上，也可以通过 RESTful 、 stock 或网络接口与远程 Docker 服务端进行通信。 这张图展示了 Docker 客户端、服务端和 Docker 仓库（即 Docker Hub 和 Docker Cloud ），默认情况下Docker 会在 Docker 中央仓库寻找镜像文件，这种利用仓库管理镜像的设计理念类似于 Git ，当然这个仓库是可以通过修改配置来指定的，甚至我们可以创建我们自己的私有仓库。]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis]]></title>
    <url>%2F2019%2F12%2F09%2Fredis%2F</url>
    <content type="text"><![CDATA[# redis相关知识]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql索引底层数据结构与算法]]></title>
    <url>%2F2019%2F10%2F08%2FMysql%E7%B4%A2%E5%BC%95%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Mysql索引底层数据结构与算法1数据结构：B+trees 非叶子节点不存储data 只存储key 叶子节点不存储指针 Mysql的存储引擎 MyISAM (非聚集索引) .frm 存储表结构 .MYD 表数据 .MYI 索引 b+trees InnoDB(聚集索引 ==主键索引).frm表结构 .ibd 索引+数据 mysql 节点默认16k 为什么InnDB表必须有主键，并且推荐使用整性的自增主键？ 使用UUID当做主键存在的问题：1.占用空间、效率低 UUID还需要转换为ascii码 2.UUID会造成数的撕裂、整型递增直接在后面追加 UUID不一定递增，会造成data分裂后在插入]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Base64Utils]]></title>
    <url>%2F2019%2F09%2F26%2FBase64Utils%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144/** * 把七牛云图片转换为base64 * @author wl */public class NetToBase64 &#123; /** * 通过图片的url获取图片的base64字符串 * @param imgUrl 图片url * @return 返回图片base64的字符串 */ public static String image2Base64(String imgUrl) &#123; URL url = null; InputStream is = null; ByteArrayOutputStream outStream = null; HttpURLConnection httpUrl = null; try&#123; url = new URL(imgUrl); httpUrl = (HttpURLConnection) url.openConnection(); httpUrl.connect(); httpUrl.getInputStream(); is = httpUrl.getInputStream(); outStream = new ByteArrayOutputStream(); //创建一个Buffer字符串 byte[] buffer = new byte[1024]; //每次读取的字符串长度，如果为-1，代表全部读取完毕 int len = 0; //使用一个输入流从buffer里把数据读取出来 while( (len=is.read(buffer)) != -1 )&#123; //用输出流往buffer里写入数据，中间参数代表从哪个位置开始读，len代表读取的长度 outStream.write(buffer, 0, len); &#125; // 对字节数组Base64编码 return Base64Util.encode(outStream.toByteArray()); &#125;catch (Exception e) &#123; e.printStackTrace(); &#125; finally&#123; if(is != null) &#123; try &#123; is.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if(outStream != null) &#123; try &#123; outStream.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if(httpUrl != null) &#123; httpUrl.disconnect(); &#125; &#125; return imgUrl; &#125; /** * base64 to image * @param BASE64str bas64字符串 * @param path 存储地址 * @param ext 图片后缀 * @return 存储地址 */ public static String BASE64CodeToBeImage(String BASE64str,String path,String ext)&#123; File fileDir = new File(path); if (!fileDir.exists()) &#123; fileDir.setWritable(true); fileDir.mkdirs(); &#125; //文件名称 String uploadFileName =new Date().getTime()+ "."+ext; File targetFile = new File(path, uploadFileName); BASE64Decoder decoder = new BASE64Decoder(); try(OutputStream out = new FileOutputStream(targetFile))&#123; byte[] b = decoder.decodeBuffer(BASE64str); for (int i = 0; i &lt;b.length ; i++) &#123; if (b[i] &lt;0) &#123; b[i]+=256; &#125; &#125; out.write(b); out.flush(); return path+"/"+uploadFileName+"."+ext; &#125;catch (Exception e)&#123; e.printStackTrace(); return null; &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public class Base64Util &#123; /** 14 * 字符串转图片 15 * @param base64Str 16 * @return 17 */ public static byte[] decode(String base64Str)&#123; byte[] b = null; BASE64Decoder decoder = new BASE64Decoder(); try &#123; b = decoder.decodeBuffer(replaceEnter(base64Str)); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return b; &#125; /** 30 * 图片转字符串 31 * @param image 32 * @return 33 */ public static String encode(byte[] image)&#123; BASE64Encoder decoder = new BASE64Encoder(); return replaceEnter(decoder.encode(image)); &#125; public static String encode(String uri)&#123; BASE64Encoder encoder = new BASE64Encoder(); return replaceEnter(encoder.encode(uri.getBytes())); &#125; /** 45 * 46 * @path 图片路径 47 * @return 48 */ public static byte[] imageTobyte(String path)&#123; byte[] data = null; FileImageInputStream input = null; try &#123; input = new FileImageInputStream(new File(path)); ByteArrayOutputStream output = new ByteArrayOutputStream(); byte[] buf = new byte[1024]; int numBytesRead = 0; while((numBytesRead = input.read(buf)) != -1)&#123; output.write(buf, 0, numBytesRead); &#125; data = output.toByteArray(); output.close(); input.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return data; &#125; public static String replaceEnter(String str)&#123; String reg ="[\n-\r]"; Pattern p = Pattern.compile(reg); Matcher m = p.matcher(str); return m.replaceAll(""); &#125;&#125;]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
</search>
